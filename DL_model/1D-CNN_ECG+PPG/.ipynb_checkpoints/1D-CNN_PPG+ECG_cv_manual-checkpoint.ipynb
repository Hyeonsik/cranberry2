{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam as Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T03:21:08.588473Z",
     "start_time": "2020-11-23T03:21:08.582499Z"
    }
   },
   "outputs": [],
   "source": [
    "# folder\n",
    "BATCH_SIZE = 512\n",
    "nfold = 4  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 50\n",
    "rootdir = \"outputs {}-fold test_settingtic {} test data\".format(nfold, ntest)\n",
    "\n",
    "predirs = []\n",
    "for root, dirs, files in os.walk(rootdir):  # 하위 대상들을 recursive 하게 긁어옴\n",
    "    for filename in dirs:\n",
    "        predirs.append(filename)\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_settings\n",
    "test_settings, elites = [], []\n",
    "droprate_fnn = 0.2\n",
    "droprate_cnn = 0.2\n",
    "\n",
    "psize = 2\n",
    "fsize = 3\n",
    "for ndense in (2, 1):\n",
    "    for global_pooling in ('max', 'ave'):  # 'ave, fla\n",
    "        for nfilts in (16, 32, 64):\n",
    "            for fnodes in (256, 512, 1024):  # 128, 512, 256\n",
    "                for cnn1_nconv in (10, 11, 12, 14, ):  # 50 hz -> 3hz, 2hz\n",
    "                    for cnn2_nconv in (0, ):  # 6 hz -> 1hz 이하\n",
    "                        test_settings.append([nfilts, fnodes, cnn1_nconv, cnn2_nconv, global_pooling, ndense])\n",
    "                        \n",
    "                        \n",
    "for kernel_n1 in ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search for hyperparameter\n",
    "ntrial = 100\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    # test_setting = random.choice(test_settings)\n",
    "\n",
    "    # test_settingtic search\n",
    "    population = test_settings + elites\n",
    "    father = random.choice(population)\n",
    "    mother = random.choice(population)\n",
    "    cross_point = random.randint(0, len(father)-1)\n",
    "    test_setting = father[:cross_point] + mother[cross_point:]\n",
    "\n",
    "    # nfilts, fnodes, cnn1_nconv, global_pooling = random.choice(test_settings)\n",
    "    nfilts, fnodes, cnn1_nconv, cnn2_nconv, global_pooling, ndense = test_setting\n",
    "\n",
    "    # 이번 옵션에 대한 테스트 데이터 생성\n",
    "    test_idx = [c in test_cases for c in all_c]\n",
    "    test_x = build_x(test_idx, cnn1_nconv, cnn2_nconv)\n",
    "    test_y = all_y[test_idx]\n",
    "    test_c = all_c[test_idx]\n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir = 'c1layers={}, c2layers={}, cfilts={}, global={}, dlayers={}, dnodes={}'.format(cnn1_nconv, cnn2_nconv, nfilts, global_pooling, ndense, fnodes)\n",
    "\n",
    "    # predone = False\n",
    "    # for filename in predirs:\n",
    "    #     if filename.find(odir) > 0:  # 이미 계산 되었으면\n",
    "    #         predone = True\n",
    "    #         break\n",
    "    # if predone:\n",
    "    #     continue\n",
    "\n",
    "    odir = rootdir + '/' + odir\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    # 이번 trial 결과\n",
    "    val_errs = []\n",
    "    train_errs = []\n",
    "\n",
    "    # 이번 실험에서 사용된 test file 명과 train file 명을 저장\n",
    "    with open(odir + \"/group.csv\", \"wt\") as fo:\n",
    "        for cid in test_cases:\n",
    "            fo.write('test,{}\\n'.format(cid))\n",
    "        for icase in range(nremain):\n",
    "            fo.write('fold{},{}\\n'.format(int(icase/nremain*nfold)+1, remained_cases[icase]))\n",
    "\n",
    "    skip_this_trial = False\n",
    "    for ifold in range(nfold):\n",
    "        # 데이터 만듬\n",
    "        if nfold == 1:\n",
    "            val_cases = remained_cases[:50]\n",
    "        else:\n",
    "            val_cases = all_cases[int(ifold/nfold*nremain):int((ifold+1)/nfold*nremain)]\n",
    "\n",
    "        train_cases = []\n",
    "        for c in remained_cases:\n",
    "            if c not in val_cases:\n",
    "                train_cases.append(c)\n",
    "\n",
    "        train_idx = [c in train_cases for c in all_c]\n",
    "        train_x = build_x(train_idx, cnn1_nconv, cnn2_nconv)\n",
    "        train_y = all_y[train_idx]\n",
    "        train_c = all_c[train_idx]\n",
    "\n",
    "        val_idx = [c in val_cases for c in all_c]\n",
    "        val_x = build_x(val_idx, cnn1_nconv, cnn2_nconv)\n",
    "        val_y = all_y[val_idx]\n",
    "        val_c = all_c[val_idx]\n",
    "\n",
    "        # training set 만 sampling weight을 조정 한다.\n",
    "        # train_sw = 1 + (train_svs - 100) / 20\n",
    "        # train_sw[train_svs < 100] = 1\n",
    "\n",
    "        print('train: {} data {} cases'.format(len(train_idx), len(train_cases)))\n",
    "        print('val: {} data {} cases'.format(len(val_idx), len(val_cases)))\n",
    "        print('test: {} data {} cases\\n'.format(len(test_idx), len(test_cases)))\n",
    "\n",
    "        # 모델 만듬\n",
    "        ain = Input(shape=(train_x[0].shape[1],))\n",
    "        aout = ain\n",
    "\n",
    "        in_cnn1 = None\n",
    "        in_cnn2 = None\n",
    "        out = None\n",
    "        if cnn1_nconv > 0:\n",
    "            in_cnn1 = Input(shape=(train_x[1].shape[1], 1))\n",
    "            out_cnn1 = build_cnn(in_cnn1, nfilts, cnn1_nconv, global_pooling, psize, fsize, droprate_cnn)\n",
    "            if out_cnn1 is None:\n",
    "                skip_this_trial = True\n",
    "                break\n",
    "            if cnn2_nconv > 0:\n",
    "                in_cnn2 = Input(shape=(train_x[2].shape[1], 1))\n",
    "                out_cnn2 = build_cnn(in_cnn2, nfilts, cnn2_nconv, global_pooling, psize, fsize, droprate_cnn)\n",
    "                if out_cnn2 is None:\n",
    "                    skip_this_trial = True\n",
    "                    break\n",
    "                out = merge([aout, out_cnn1, out_cnn2], mode='concat', concat_axis=1)\n",
    "            else:\n",
    "                out = merge([aout, out_cnn1], mode='concat', concat_axis=1)\n",
    "        else:\n",
    "            out = aout\n",
    "\n",
    "        for idense in range(ndense):\n",
    "            out = Dense(fnodes, activation='relu')(out)\n",
    "            out = Dropout(droprate_fnn)(out)\n",
    "\n",
    "        out = Dense(1)(out)\n",
    "\n",
    "        if cnn1_nconv > 0:\n",
    "            if cnn2_nconv > 0:\n",
    "                inp = [ain, in_cnn1, in_cnn2]\n",
    "            else:\n",
    "                inp = [ain, in_cnn1]\n",
    "        else:\n",
    "            inp = [ain]\n",
    "\n",
    "        model = Model(input=inp, output=[out])\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "        weightcache = \"{}/model_{}.hdf5\".format(odir, ifold + 1)\n",
    "\n",
    "        hist = model.fit(train_x, train_y, validation_data=(val_x, val_y), nb_epoch=10,\n",
    "                         batch_size=batch_size, shuffle=True, #sample_weight=train_sw,\n",
    "                         callbacks=[ModelCheckpoint(filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                    EarlyStopping(monitor='val_loss', patience=1, verbose=2, mode='auto')])\n",
    "        model = keras.models.load_model(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "\n",
    "        train_err = min(hist.history['mean_absolute_error'])\n",
    "        val_err = min(hist.history['val_mean_absolute_error'])\n",
    "\n",
    "        val_errs.append(val_err)\n",
    "        train_errs.append(train_err)\n",
    "\n",
    "        del train_x\n",
    "        del train_y\n",
    "        del train_idx\n",
    "        del val_x\n",
    "        del val_y\n",
    "        del val_idx\n",
    "        del model\n",
    "\n",
    "    if skip_this_trial:\n",
    "        print('this trial skipped')\n",
    "        continue\n",
    "\n",
    "    # nfold 학습 완료\n",
    "    # 최고의 모델을 다시 읽음\n",
    "    bestfold = val_errs.index(min(val_errs))\n",
    "    mean_val_err = np.mean(val_errs)\n",
    "    mean_train_err = np.mean(train_errs)\n",
    "\n",
    "    weightcache = \"{}/model_{}.hdf5\".format(odir, bestfold + 1)\n",
    "    bestcache = \"{}/model_best.hdf5\".format(odir)\n",
    "    shutil.copy(weightcache, bestcache)\n",
    "    for ifold in range(nfold):\n",
    "        srccache = \"{}/model_{}.hdf5\".format(odir, ifold + 1, )\n",
    "        dstcache = \"{}/model_{}_{}.hdf5\".format(odir, ifold + 1, val_errs[ifold])\n",
    "        os.rename(srccache, dstcache)\n",
    "    model = keras.models.load_model(bestcache)\n",
    "\n",
    "    # test 에러를 구함\n",
    "    pred_y = model.predict(test_x)[:, 0]\n",
    "    diff_y = test_y - pred_y\n",
    "    test_err = np.mean(np.abs(diff_y))\n",
    "    loa = 100 * lib.calc_loa(diff_y, test_c) / np.mean(test_y)\n",
    "\n",
    "    # test error 를 case별로 구함\n",
    "    case_errs = []\n",
    "    for caseid in test_cases:\n",
    "        case_idx = (test_c == caseid)\n",
    "        case_real = test_y[case_idx]\n",
    "        case_pred = pred_y[case_idx]\n",
    "        case_err = np.mean(np.abs(case_real - case_pred))\n",
    "        case_errs.append((caseid, case_err))\n",
    "\n",
    "        # case별 파일을 저장함\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.xlim([0, len(case_real)])\n",
    "        plt.ylim([20, 200])\n",
    "        plt.plot(np.arange(0, len(case_real)), case_real)\n",
    "        plt.plot(np.arange(0, len(case_pred)), case_pred)\n",
    "        plt.savefig(\"{}/{}.png\".format(odir, caseid), format='png')\n",
    "        plt.close()\n",
    "\n",
    "    case_errs = sorted(case_errs, key=lambda o: -o[1])\n",
    "\n",
    "    with open(odir + '/result.csv', 'wt') as f:\n",
    "        f.write('id,mae\\n')\n",
    "        for o in case_errs:\n",
    "            f.write(\"{},{}\\n\".format(o[0], o[1]))\n",
    "\n",
    "    newdir = \"{}/va {:.3f}, {}, tr {:.3f}, te {:.3f}, loa {:.1f}\".format(os.path.dirname(odir), mean_val_err, os.path.basename(odir), mean_train_err, test_err, loa)\n",
    "    if mean_val_err < 7:\n",
    "        newdir += ' @@@@'\n",
    "        if len(elites) >= len(test_settings):\n",
    "            del elites[0]  # 오래된 엘리트부터 제거\n",
    "        elites.append(test_setting)\n",
    "\n",
    "    os.rename(odir, newdir)\n",
    "\n",
    "    del test_idx\n",
    "    del test_x\n",
    "    del test_y\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
